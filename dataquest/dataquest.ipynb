{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Create a simple spam filter using Naive Bayes classification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In classification tasks, as in many other things, complicated does not necessarily equal better. While it is possible and sometimes desirable to use things like neural network or support vector machines to classify observations, Naive Bayes - one of the simplest classification methods-  often outperforms its more complicated counterparts. This seems to be particularly true when it comes to text classification (see [here](http://cogprints.org/6708/1/4-1-16-23.pdf) and [here](https://thesai.org/Downloads/Volume4No11/Paper_5-Performance_Comparison_between_Na%C3%AFve_Bayes.pdf), for example).</p>\n",
    "\n",
    "<p>In this post, we’ll first look at the concept of conditional probability, which is the basis of the Naive Bayes classifier. Then we’ll use the scikit-learn library to train a classifier to detect spam emails. Finally, we’ll explore a few different ways a Naive Bayes model can be fit depending on what assumptions we want to make about our data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conditional Probability</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>You’ve probably heard of the duck test:</p>\n",
    "\n",
    "<blockquote>“If it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck.”</blockquote>\n",
    "\n",
    "<p>That’s conditional probability. For any given set of animals, the probability that any one animal is a duck can be estimated by:</p>\n",
    "\n",
    "<blockquote>P(is a duck) = [total # of ducks] / [total # of animals]</blockquote>\n",
    "\n",
    "<p>And the probability that any one animal, say, swims like a duck, can be estimated by:</p>\n",
    "\n",
    "<blockquote>P(swims like a duck) = [total # of animals who swim like a duck] / [total # of animals]</blockquote>\n",
    "\n",
    "<p>In this example, conditional probability is the probability that an animal is a duck if we already know that the animal swims like a duck. That’s calculated by dividing the joint probability (the percentage of animals that both are a duck and swim like a duck) but the percentage of animals who swim like a duck. So:</p>\n",
    "\n",
    "<blockquote>P(is a duck, given that it swims like a duck) = P(is a duck and swims like a duck) / P(swims like a duck)</blockquote>\n",
    "\n",
    "<p>In other words, reduce our universe from the total number of animals to just the percentage of animals that swim like a duck. By using a percentage instead of a count, we’re continuing to use the original probabilities (based on total number of animals), but we’re forcing our calculation to only consider those animals that meet the condition.</p>\n",
    "\n",
    "<p>What does this have to do with classification? Well, if we have a lot of animals, and we want to figure out which animals are ducks, we can combined conditional probabilities. For example, if one animal looks like a duck, swims like a duck, and quacks like a duck, then then:</p>\n",
    "\n",
    "<blockquote>P(animal is a duck) = P(is a duck, given that it looks like a duck) `* P(is a duck, given that it swims like a duck) `* P(is a duck, given that it quacks like a duck)</blockquote>\n",
    "\n",
    "<p>If, instead of a duck, we thought it was possible that the animal was a goose, we could calculate another probability:</p>\n",
    "\n",
    "<blockquote>P(animal is a goose) = P(is a goose, given that it looks like a duck) `* P(is a goose, given that it swims like a duck) `* P(is a goose, given that it quacks like a duck)</blockquote>\n",
    "\n",
    "<p>A goose could conceivably look a little like a duck, and could very likely swim like a duck, must most likely wouldn’t quack like a duck. Therefore, if an animal looked, swam, and quacked like a duck, the probability of it being a duck would be higher than the probability of it being a goose, and therefore we would classify it as a duck.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spam filtering</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we will look at how we could automate the above classification procedure in Python. I don’t have an animals dataset, so instead of ducks and geese we’ll look at spam (unsolicited bulk emails) and ham (emails that aren’t spam). There are a number corpuses of spam/ham emails. We’ll grab a few from the Enron dataset:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cStringIO import StringIO\n",
    "import tarfile\n",
    "import requests\n",
    "\n",
    "url = 'http://www.aueb.gr/users/ion/data/enron-spam/preprocessed/enron1.tar.gz'\n",
    "response = requests.get(url)\n",
    "\n",
    "tar = tarfile.open(mode=\"r:gz\", fileobj=StringIO(response.content))\n",
    "spam = [tar.extractfile(m).read() for m in tar.getmembers() if 'spam.txt' in m.name]\n",
    "ham = [tar.extractfile(m).read() for m in tar.getmembers() if 'ham.txt' in m.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The above code requests a zipped tar file containing text files, each file containing a single email. We use the filename for each text file to create separate lists of spam and ham.</p>\n",
    "\n",
    "<p>Now that we have the texts, we need to pull out word counts. The number of times a word appears in each email will be our equivalent of “looks like a duck”, “quacks like a duck”, etc.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(\n",
    "   max_df=0.95,\n",
    "   min_df=2,\n",
    "   max_features=1000,\n",
    "   stop_words='english',\n",
    "   lowercase=True,\n",
    "   encoding='utf-8',\n",
    "   decode_error='replace'\n",
    ")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(spam + ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The `CountVectorizer` class from scikit-learn turns a list of texts into a sparse document-by-word matrix. The class has a lot of parameters, so it makes sense to explain the ones we used above:</p>\n",
    "<ul>\n",
    "<li>`max_df=0.95`: only take words that appear in 95% or fewer of the total documents (if a word appears in every document, then it probably won’t help us differentiate spam documents from ham documents).</li>\n",
    "<li>`min_df=2`: only take words that appear in at least two documents (if a word appears in only one document, it probably won’t help us differentiate anything).</li>\n",
    "<li>`max_features=1000`: extract the 1000 most frequent words.</li>\n",
    "<li>`stop_words=’english’`: remove words that occur so commonly in English that they probably won’t help us differentiate documents; these include words like “and” and “the”.</li>\n",
    "<li>`lowercase=True`: make all words lowercase before counting; this prevents us from treating, say, “Hello” and “hello” as two separate words just because one happened to be placed at the first of a sentence and the other one in the middle of the sentence.</li>\n",
    "<li>`encoding=’utf-8’`: this tells CountVectorizer the range of characters that should be considered valid.</li>\n",
    "<li>`decode_error=’replace’`: this tells CountVectorizer to replace invalid characters with a meaningless valid character - if we planned to use our spam filter in real life, this would probably be a bad idea, but for the purposes of our example here it allows us to not spend too much time figuring out how to clean our data.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Now, let’s put our data into a Pandas DataFrame so we can explore it a little bit:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame, Series\n",
    "\n",
    "df = DataFrame(tf.todense(), columns=tf_vectorizer.get_feature_names())\n",
    "is_spam = Series(([1.0] * len(spam)) + ([0.0] * len(ham)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The above code creates a DataFrame from the document-word matrix, and labels each column with the appropriate word from the vectorizer. It also creates a Series indicating 1.0 if a document was identified as spam, and 0.0 if identified as ham. We can then do things like look at the 10 most-used words.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ect      13900\n",
      "hou       7289\n",
      "enron     6555\n",
      "2000      4386\n",
      "com       3710\n",
      "gas       3034\n",
      "deal      2827\n",
      "meter     2459\n",
      "00        2404\n",
      "cc        2371\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>However, many words appear multiple times within a single document. If we want to see which words are most-represented across documents:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000      0.299691\n",
      "enron     0.282676\n",
      "thanks    0.276875\n",
      "cc        0.249227\n",
      "gas       0.219644\n",
      "know      0.215004\n",
      "hpl       0.212297\n",
      "10        0.206883\n",
      "com       0.202436\n",
      "daren     0.199149\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print df.gt(0).mean().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can also use our series of spam vs. ham indicators to see which words are most-represented across spam documents:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http      0.279902\n",
      "www       0.145649\n",
      "com       0.132723\n",
      "email     0.129420\n",
      "best      0.124767\n",
      "click     0.114373\n",
      "money     0.111050\n",
      "online    0.106721\n",
      "free      0.099301\n",
      "prices    0.091473\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pct_sorted = df.gt(0).groupby(is_spam).mean().transpose()\n",
    "print (pct_sorted[1.0] - pct_sorted[0.0]).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>And which words are most-represented across ham documents:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enron       -0.398148\n",
      "2000        -0.384553\n",
      "cc          -0.341645\n",
      "thanks      -0.312041\n",
      "hpl         -0.299020\n",
      "gas         -0.288710\n",
      "daren       -0.280501\n",
      "pm          -0.266266\n",
      "forwarded   -0.246610\n",
      "ect         -0.240102\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print (pct_sorted[1.0] - pct_sorted[0.0]).sort_values(ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Looking at the list a lot of the spammy words do, in fact, look spamy ('prices', 'clicks', 'money', 'online', etc.), and many of the hammy words look like exactly what we would expect to find in emails at Enron in the year 2000 (such as the words \"Enron\" and \"2000\").</p>\n",
    "\n",
    "<p>But 1000 words is a lot. We don't want to have to eyeball percentages per word to get a classification. There is where a Naive Bayes implementation comes in handy, and scikit-learn has just such an implemenation. First, though, let's look a toy example.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Toy Example</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Our full data set is 5172 emails by 1000 words. Let's take a small snippet of that to run through the details of calculating a Naive Bayes classification:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose just a few words\n",
    "keep_cols = ['new', 'read', 'set', 'thanks', '12', 'nomination']  \n",
    "\n",
    "# grab five spam and five ham emails, and reduce the columns to only those chosen above\n",
    "df_toy = df.groupby(is_spam).head(5).loc[:, keep_cols] \n",
    "\n",
    "# filter the spam indicator to only include the emails kept in the predictors dataset\n",
    "is_spam_toy = is_spam.loc[df_toy.index]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we can train a classifier on the toy data. First count up the total number of emails - this is just the number of rows in your data set. Then separate the emails into spam and ham. I used a dictionary to do that:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: array([[0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 1, 1, 8, 1],\n",
      "       [0, 0, 0, 0, 0, 2],\n",
      "       [0, 0, 0, 1, 5, 0],\n",
      "       [0, 0, 0, 1, 5, 1]]), 1.0: array([[1, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 2, 0, 0, 0],\n",
      "       [2, 1, 1, 0, 2, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# count up the total number of emails\n",
    "email_count = float(df_toy.shape[0])\n",
    "\n",
    "# separate emails into spam and ham (remember 0 == ham, 1 == spam)\n",
    "separated = df_toy.groupby(is_spam).apply(lambda p: p.values).to_dict()\n",
    "print separated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we need to calculate the prior for each class. That's just the number of emails in each class divided by the total number of rows.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class priors: {0.0: 0.5, 1.0: 0.5}\n",
      "log of class priors {0.0: 0.40546510810816438, 1.0: 0.40546510810816438}\n"
     ]
    }
   ],
   "source": [
    "from numpy import log\n",
    "\n",
    "class_prior = {k: len(v) / email_count for k, v in separated.items()}\n",
    "class_log_prior = {k: log(v + 1) for k, v, in class_prior.items()}\n",
    "print 'class priors:', class_prior\n",
    "print 'log of class priors', class_log_prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can see that the prior for both classes is 0.5. That's because we selected an equal number of emails from each class for our toy example. Notice also that I took the log of the priors. That's a safeguard. When you have data sets where most of the values are zero (common in text classification problems), your percentages can get so small that they truncated to zero. That's because floating point numbers don't have infinite precision. Logging those numbers avoids that problem. Notice that I actually took the log of one plus the prior. That's because the log of zero is undefined. Adding the one keeps keeps that from becoming a problem.</p>\n",
    "\n",
    "<p>So now we need the word probabilities. First, we count up the total number of times each word appears in each class. Then we count up the number of total words per class. Then we divide the word counts by the total words and, as before, take the log of one plus that number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of time each word appears in each class\n",
    "word_counts = {k: v.sum(axis=0) for k, v in separated.items()}\n",
    "\n",
    "# Number of times any word appears in each class\n",
    "total_words_per_class = {k: v.sum() for k, v in word_counts.items()}\n",
    "\n",
    "# Word counts divided by total words\n",
    "feature_prob = {k: v / float(total_words_per_class[k]) for k, v in word_counts.items()}\n",
    "\n",
    "# Log to avoid floating-point underflow\n",
    "feature_log_prob = {k: log(v + 1) for k, v in feature_prob.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>So now we have all the numbers we need to classify the emails. In this case, we're going to predict the classifications of the emails we used to train the classifier. Normally, that would be a really bad idea - it leads to overfitting, where the model performs better than we can expect it to in real life because it essentially had the answers to the problem beforehand. We'll do things right when we classify the whole dataset below, but for right now, predicting the test data allows us to see how the classifications work.</p>\n",
    "\n",
    "<p>The first step is to calculate the log probability for each email for each class (ham vs. spam). Classification involves chosing whichever log probability is higher for each email.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      original  predictions\n",
      "0          1.0          1.0\n",
      "1          1.0          0.0\n",
      "2          1.0          1.0\n",
      "3          1.0          1.0\n",
      "4          1.0          1.0\n",
      "1500       0.0          0.0\n",
      "1501       0.0          0.0\n",
      "1502       0.0          0.0\n",
      "1503       0.0          0.0\n",
      "1504       0.0          0.0\n"
     ]
    }
   ],
   "source": [
    "from pandas import concat\n",
    "\n",
    "# create a Series of log probabilities. There will be as many Series as their are classes\n",
    "# and each series will have as many values as there are emails.\n",
    "predict_log_prob = {k: (df_toy * v + class_log_prior[k]).sum(axis=1) \n",
    "                    for k, v in feature_log_prob.items()}\n",
    "\n",
    "# Concatenate the Series into a dataframe, with one column per class. Go through each\n",
    "# row and pick the column that represents the higher log probability.\n",
    "predict_class = concat(predict_log_prob, axis=1).apply(lambda r: r.argmax(), axis=1)\n",
    "\n",
    "# show original data and predictions side by side\n",
    "comparison = concat([is_spam_toy, predict_class], keys=['original', 'predictions'], axis=1)\n",
    "\n",
    "print comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can see that the classifier picked the right classificaton in all but one case. Because we were dealing with a very small data set, and because some spam contains words that often appear in ham while some ham contains words that often appear in spam, we wouldn't expect performance to be perfect. Again, the performance we see here is probably much better than what we would expect if we tried to predict emails that weren't included in the training set. That's what we'll do in the next section.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Full Implmentation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "# randomly select 80% of the dataset to use in training the classifier\n",
    "test_indices = choice(is_spam.index, int(is_spam.shape[0] * 0.8), replace=False)\n",
    "is_test_record = df.index.isin(test_indices)\n",
    "\n",
    "# subset the full document-by-word DataFrame and spam/ham indicator to only include training indices\n",
    "train_X = df.loc[is_test_record, :]\n",
    "test_X = df.loc[~is_test_record, :]\n",
    "\n",
    "# subset the DataFrame and indicator Series again to exclude the training indices\n",
    "train_y = is_spam.loc[is_test_record]\n",
    "test_y = is_spam.loc[~is_test_record]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>It's always important when training a model to hold out some data for the purpose of testing the model's fit. The model learns rules based on the training data. Measuring how good the model learned those rules by using that same training data can result in overfitting, which makes the model look a lot more successful than it really is.</p>\n",
    "\n",
    "<p>So, now that we have a training set and a testing set, we can run the actual model:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# fit the model\n",
    "model = MultinomialNB().fit(train_X, train_y)\n",
    "\n",
    "# predict classifications based on the test data\n",
    "predicted = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we can use the testing outcomes that we witheld do see how well our model performed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.94      0.95       729\n",
      "        1.0       0.86      0.92      0.89       306\n",
      "\n",
      "avg / total       0.93      0.93      0.93      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Remember, 0.0 means an email was ham, and 1.0 means the email was spam. Precision is the percentage of model-assigned labels that were ultimately accurate (so the percentage of documents that the model said were ham or spam that were actually ham or spam, respectively). Recall is the percentage of actual values that the model correctly labelled (so the percentage of ham or spam documents that the model correctly identified as ham or spam, respectively). The f1-score is the [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean) of the precision and recall - just a way of reducing those two measure to a single number. The \"support\" column tells you how many records were in each category.</p>\n",
    "\n",
    "<p>One other way to look at this is to use a confusion matrix:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[683  46]\n",
      " [ 25 281]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(test_y, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The rows are the actual labels (in sorted order - so ham(0.0) is the first row and spam (1.0) is the second row), and the columns are the model's predicted labels. So in 710 cases, the model said the email was ham and it actually was ham, and in 257 cases, the model said it was spam and it actually was spam. However, in 48 cases, the model said it was spam while it was actually ham, and in 20 cases, the model said it was ham when it was actually spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>A few other things you should know</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The spam filter example we showed above used the MultinomialNB class from scikit-learn. Multinomial data is more-or-less count data, and fits well with the ducks vs. geese example that we used to describe conditional probability: it essentially involves counting up the number of occurences to determine the probabilities.</p>\n",
    "\n",
    "<p>While technically not multinomial data, text classifiers often perform better when they use a tf-idf (term frequency-inverse document frequency) transformation instead of raw word counts. A tf-idf transformation divides the number of times a word occurs in a document by the number of times that word occurs across all of the documents. That ends up down-weighting words that might appear more often in a document for no other reason than that they are more frequently used in general.</p>\n",
    "\n",
    "<p>There are also options for using Gaussian or binary data. The Bernoulli Naive Bayes implementaton (`BernoulliNB` in scikit-learn) takes only binary data and treats each variant as its own attribute. In other words, if we go back to our duck and goose discussion, a Bernoulli approach would treats \"quacks like a duck\" and \"does not quack like a duck\" as two different attributes. The `GaussianNB` class in scikit-learn takes normally-distributed data (say, height and weight), calculates the means and standard distribution of of those features within each category (say, male and female), and then determines where each actual value fits within a probability density. So, for example, the more a person's height and weight fall near the mean heigh and weight for men, the more likely the classifier will be to predict that that person is male.</p>\n",
    "\n",
    "<p>All of the above should give you the basic background and syntax necessary to identify sitauations where a Naive Bayes classifier could come in handy, and to set up your data and analysis pipelines to train and validate the model.</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
