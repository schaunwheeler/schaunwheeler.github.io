{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Create a simple spam filter using Naive Bayes classification</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In classification tasks, as in many other things, complicated does not necessarily equal better. While it is possible and sometimes desirable to use things like neural network or support vector machines to classify observations, Naive Bayes - one of the simplest classification methods-  often outperforms its more complicated counterparts. This seems to be particularly true when it comes to text classification (see [here](http://cogprints.org/6708/1/4-1-16-23.pdf) and [here](https://thesai.org/Downloads/Volume4No11/Paper_5-Performance_Comparison_between_Na%C3%AFve_Bayes.pdf), for example).</p>\n",
    "\n",
    "<p>In this post, we’ll first look at the concept of conditional probability, which is the basis of the Naive Bayes classifier. Then we’ll use the scikit-learn library to train a classifier to detect spam emails. Finally, we’ll explore a few different ways a Naive Bayes model can be fit depending on what assumptions we want to make about our data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conditional Probability</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>You’ve probably heard of the duck test:</p>\n",
    "\n",
    "<blockquote>“If it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck.”</blockquote>\n",
    "\n",
    "<p>That’s conditional probability. For any given set of animals, the probability that any one animal is a duck can be estimated by:</p>\n",
    "\n",
    "<blockquote>P(is a duck) = [total # of ducks] / [total # of animals]</blockquote>\n",
    "\n",
    "<p>And the probability that any one animal, say, swims like a duck, can be estimated by:</p>\n",
    "\n",
    "<blockquote>P(swims like a duck) = [total # of animals who swim like a duck] / [total # of animals]</blockquote>\n",
    "\n",
    "<p>In this example, conditional probability is the probability that an animal is a duck if we already know that the animal swims like a duck. That’s calculated by dividing the joint probability (the percentage of animals that both are a duck and swim like a duck) but the percentage of animals who swim like a duck. So:</p>\n",
    "\n",
    "<blockquote>P(is a duck, given that it swims like a duck) = P(is a duck and swims like a duck) / P(swims like a duck)</blockquote>\n",
    "\n",
    "<p>In other words, reduce our universe from the total number of animals to just the percentage of animals that swim like a duck. By using a percentage instead of a count, we’re continuing to use the original probabilities (based on total number of animals), but we’re forcing our calculation to only consider those animals that meet the condition.</p>\n",
    "\n",
    "<p>What does this have to do with classification? Well, if we have a lot of animals, and we want to figure out which animals are ducks, we can combined conditional probabilities. For example, if one animal looks like a duck, swims like a duck, and quacks like a duck, then then:</p>\n",
    "\n",
    "<blockquote>P(animal is a duck) = P(is a duck, given that it looks like a duck) `* P(is a duck, given that it swims like a duck) `* P(is a duck, given that it quacks like a duck)</blockquote>\n",
    "\n",
    "<p>If, instead of a duck, we thought it was possible that the animal was a goose, we could calculate another probability:</p>\n",
    "\n",
    "<blockquote>P(animal is a goose) = P(is a goose, given that it looks like a duck) `* P(is a goose, given that it swims like a duck) `* P(is a goose, given that it quacks like a duck)</blockquote>\n",
    "\n",
    "<p>A goose could conceivably look a little like a duck, and could very likely swim like a duck, must most likely wouldn’t quack like a duck. Therefore, if an animal looked, swam, and quacked like a duck, the probability of it being a duck would be higher than the probability of it being a goose, and therefore we would classify it as a duck.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spam filtering</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we will look at how we could automate the above classification procedure in Python. I don’t have an animals dataset, so instead of ducks and geese we’ll look at spam (unsolicited bulk emails) and ham (emails that aren’t spam). There are a number corpuses of spam/ham emails. We’ll grab a few from the Enron dataset:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cStringIO import StringIO\n",
    "import tarfile\n",
    "import requests\n",
    "\n",
    "url = 'http://www.aueb.gr/users/ion/data/enron-spam/preprocessed/enron1.tar.gz'\n",
    "response = requests.get(url)\n",
    "\n",
    "tar = tarfile.open(mode=\"r:gz\", fileobj=StringIO(response.content))\n",
    "spam = [tar.extractfile(m).read() for m in tar.getmembers() if 'spam.txt' in m.name]\n",
    "ham = [tar.extractfile(m).read() for m in tar.getmembers() if 'ham.txt' in m.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The above code requests a zipped tar file containing text files, each file containing a single email. We use the filename for each text file to create separate lists of spam and ham.</p>\n",
    "\n",
    "<p>Now that we have the texts, we need to pull out word counts. The number of times a word appears in each email will be our equivalent of “looks like a duck”, “quacks like a duck”, etc.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(\n",
    "   max_df=0.95,\n",
    "   min_df=2,\n",
    "   max_features=1000,\n",
    "   stop_words='english',\n",
    "   lowercase=True,\n",
    "   encoding='utf-8',\n",
    "   decode_error='replace'\n",
    ")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(spam + ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The `CountVectorizer` class from scikit-learn turns a list of texts into a sparse document-by-word matrix. The class has a lot of parameters, so it makes sense to explain the ones we used above:</p>\n",
    "<ul>\n",
    "<li>`max_df=0.95`: only take words that appear in 95% or fewer of the total documents (if a word appears in every document, then it probably won’t help us differentiate spam documents from ham documents).</li>\n",
    "<li>`min_df=2`: only take words that appear in at least two documents (if a word appears in only one document, it probably won’t help us differentiate anything).</li>\n",
    "<li>`max_features=1000`: extract the 1000 most frequent words.</li>\n",
    "<li>`stop_words=’english’`: remove words that occur so commonly in English that they probably won’t help us differentiate documents; these include words like “and” and “the”.</li>\n",
    "<li>`lowercase=True`: make all words lowercase before counting; this prevents us from treating, say, “Hello” and “hello” as two separate words just because one happened to be placed at the first of a sentence and the other one in the middle of the sentence.</li>\n",
    "<li>`encoding=’utf-8’`: this tells CountVectorizer the range of characters that should be considered valid.</li>\n",
    "<li>`decode_error=’replace’`: this tells CountVectorizer to replace invalid characters with a meaningless valid character - if we planned to use our spam filter in real life, this would probably be a bad idea, but for the purposes of our example here it allows us to not spend too much time figuring out how to clean our data.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Now, let’s put our data into a Pandas DataFrame so we can explore it a little bit:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame, Series\n",
    "\n",
    "df = DataFrame(tf.todense(), columns=tf_vectorizer.get_feature_names())\n",
    "is_spam = Series(([1.0] * len(spam)) + ([0.0] * len(ham)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The above code creates a DataFrame from the document-word matrix, and labels each column with the appropriate word from the vectorizer. It also creates a Series indicating 1.0 if a document was identified as spam, and 0.0 if identified as ham. We can then do things like look at the 10 most-used words.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ect      13900\n",
      "hou       7289\n",
      "enron     6555\n",
      "2000      4386\n",
      "com       3710\n",
      "gas       3034\n",
      "deal      2827\n",
      "meter     2459\n",
      "00        2404\n",
      "cc        2371\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>However, many words appear multiple times within a single document. If we want to see which words are most-represented across documents:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000      0.299691\n",
      "enron     0.282676\n",
      "thanks    0.276875\n",
      "cc        0.249227\n",
      "gas       0.219644\n",
      "know      0.215004\n",
      "hpl       0.212297\n",
      "10        0.206883\n",
      "com       0.202436\n",
      "daren     0.199149\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print df.gt(0).mean().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can also use our series of spam vs. ham indicators to see which words are most-represented across spam documents:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http      0.279902\n",
      "www       0.145649\n",
      "com       0.132723\n",
      "email     0.129420\n",
      "best      0.124767\n",
      "click     0.114373\n",
      "money     0.111050\n",
      "online    0.106721\n",
      "free      0.099301\n",
      "prices    0.091473\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pct_sorted = df.gt(0).groupby(is_spam).mean().transpose()\n",
    "print (pct_sorted[1.0] - pct_sorted[0.0]).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>And which words are most-represented across ham documents:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enron       -0.398148\n",
      "2000        -0.384553\n",
      "cc          -0.341645\n",
      "thanks      -0.312041\n",
      "hpl         -0.299020\n",
      "gas         -0.288710\n",
      "daren       -0.280501\n",
      "pm          -0.266266\n",
      "forwarded   -0.246610\n",
      "ect         -0.240102\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print (pct_sorted[1.0] - pct_sorted[0.0]).sort_values(ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Looking at the list a lot of the spammy words do, in fact, look spamy ('prices', 'clicks', 'money', 'online', etc.), and many of the hammy words look like exactly what we would expect to find in emails at Enron in the year 2000 (such as the words \"Enron\" and \"2000\").</p>\n",
    "\n",
    "<p>But 1000 words is a lot. We don't want to have to eyeball percentages per word to get a classification. There is where a Naive Bayes implementation comes in handy, and scikit-learn has just such an implemenation. First, though, let's set up some training data and test data:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "# randomly select 80% of the dataset to use in training the classifier\n",
    "test_indices = choice(is_spam.index, int(is_spam.shape[0] * 0.8), replace=False)\n",
    "is_test_record = df.index.isin(test_indices)\n",
    "\n",
    "# subset the full document-by-word DataFrame and spam/ham indicator to only include training indices\n",
    "train_X = df.loc[is_test_record, :]\n",
    "test_X = df.loc[~is_test_record, :]\n",
    "\n",
    "# subset the DataFrame and indicator Series again to exclude the training indices\n",
    "train_y = is_spam.loc[is_test_record]\n",
    "test_y = is_spam.loc[~is_test_record]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>It's always important when training a model to hold out some data for the purpose of testing the model's fit. The model learns rules based on the training data. Measuring how good the model learned those rules by using that same training data can result in overfitting, which makes the model look a lot more successful than it really is.</p>\n",
    "\n",
    "<p>So, now that we have a training set and a testing set, we can run the actual model:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# fit the model\n",
    "model = MultinomialNB().fit(train_X, train_y)\n",
    "\n",
    "# predict classifications based on the test data\n",
    "predicted = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we can use the testing outcomes that we witheld do see how well our model performed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.93      0.95       746\n",
      "        1.0       0.84      0.90      0.87       289\n",
      "\n",
      "avg / total       0.93      0.92      0.93      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Remember, 0.0 means an email was ham, and 1.0 means the email was spam. Precision is the percentage of model-assigned labels that were ultimately accurate (so the percentage of documents that the model said were ham or spam that were actually ham or spam, respectively). Recall is the percentage of actual values that the model correctly labelled (so the percentage of ham or spam documents that the model correctly identified as ham or spam, respectively). The f1-score is the [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean) of the precision and recall - just a way of reducing those two measure to a single number. The \"support\" column tells you how many records were in each category.</p>\n",
    "\n",
    "<p>One other way to look at this is to use a confusion matrix:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[697  49]\n",
      " [ 29 260]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(test_y, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The rows are the actual labels (in sorted order - so ham(0.0) is the first row and spam (1.0) is the second row), and the columns are the model's predicted labels. So in 710 cases, the model said the email was ham and it actually was ham, and in 257 cases, the model said it was spam and it actually was spam. However, in 48 cases, the model said it was spam while it was actually ham, and in 20 cases, the model said it was ham when it was actually spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>A few other things you should know</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The spam filter example we showed above used the MultinomialNB class from scikit-learn. Multinomial data is more-or-less count data, and fits well with the ducks vs. geese example that we used to describe conditional probability: it essentially involves counting up the number of occurences to determine the probabilities.</p>\n",
    "\n",
    "<p>While technically not multinomial data, text classifiers often perform better when they use a tf-idf (term frequency-inverse document frequency) transformation instead of raw word counts. A tf-idf transformation divides the number of times a word occurs in a document by the number of times that word occurs across all of the documents. That ends up down-weighting words that might appear more often in a document for no other reason than that they are more frequently used in general.</p>\n",
    "\n",
    "<p>There are also options for using Gaussian or binary data. The Bernoulli Naive Bayes implementaton (`BernoulliNB` in scikit-learn) takes only binary data and treats each variant as its own attribute. In other words, if we go back to our duck and goose discussion, a Bernoulli approach would treats \"quacks like a duck\" and \"does not quack like a duck\" as two different attributes. The `GaussianNB` class in scikit-learn takes normally-distributed data (say, height and weight), calculates the means and standard distribution of of those features within each category (say, male and female), and then determines where each actual value fits within a probability density. So, for example, the more a person's height and weight fall near the mean heigh and weight for men, the more likely the classifier will be to predict that that person is male.</p>\n",
    "\n",
    "<p>All of the above should give you the basic background and syntax necessary to identify sitauations where a Naive Bayes classifier could come in handy, and to set up your data and analysis pipelines to train and validate the model.</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
